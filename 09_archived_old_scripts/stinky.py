import os
import psycopg2
import json
from psycopg2.extras import Json
import openai
import re
import argparse
import csv
import time

# --- Configuration ---
api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    api_key = "YOUR_OPENAI_API_KEY_HERE"
if not api_key:
    raise Exception("CRITICAL: OPENAI_API_KEY is not set.")
openai.api_key = api_key

PG_HOST = "localhost"
PG_PORT = "5432"
PG_DATABASE = "price_comparison_app_v2"
PG_USER = "postgres"
PG_PASSWORD = "025655358"

BATCH_SIZE = 500
CACHE_FILE = "enrichment_cache.json"

PRODUCT_TYPE_MAP = {
    "Shampoo": ["◊©◊û◊§◊ï", "◊©◊û◊§◊ï ◊†◊í◊ì ◊ß◊©◊ß◊©◊ô◊ù", "◊©◊û◊§◊ï ◊ô◊ë◊©", "◊©◊û◊§◊ï ◊ú◊©◊ô◊¢◊® ◊¶◊ë◊ï◊¢", "◊©◊û◊§◊ï ◊ú◊™◊ô◊†◊ï◊ß◊ï◊™"],
    "Conditioner": ["◊û◊®◊õ◊ö", "◊ß◊ï◊†◊ì◊ô◊©◊ô◊†◊®", "◊û◊®◊õ◊ö ◊©◊ô◊¢◊®", "◊û◊°◊õ◊î ◊ú◊©◊ô◊¢◊®"],
    "Body Wash": ["◊í'◊ú ◊®◊ó◊¶◊î", "◊°◊ë◊ï◊ü ◊í◊ï◊£", "◊ê◊ú ◊®◊ó◊¶◊î", "◊°◊ë◊ï◊ü ◊†◊ï◊ñ◊ú◊ô ◊ú◊í◊ï◊£", "◊™◊ó◊ú◊ô◊ë ◊®◊ó◊¶◊î"],
    "Deodorant": ["◊ì◊ê◊ï◊ì◊ï◊®◊†◊ò", "◊ì◊ê◊ï", "◊ê◊†◊ò◊ô◊§◊®◊°◊§◊ô◊®◊†◊ò", "◊ê◊†◊ò◊ô ◊§◊®◊°◊§◊ô◊®◊†◊ò", "◊°◊ò◊ô◊ß ◊ì◊ê◊ï◊ì◊ï◊®◊†◊ò", "◊®◊ï◊ú ◊ê◊ï◊ü"],
    "Toothpaste": ["◊û◊©◊ó◊™ ◊©◊ô◊†◊ô◊ô◊ù"], "Toothbrush": ["◊û◊ë◊®◊©◊™ ◊©◊ô◊†◊ô◊ô◊ù", "◊û◊ë◊®◊©◊™ ◊ó◊©◊û◊ú◊ô◊™"],
    "Mouthwash": ["◊©◊ò◊ô◊§◊™ ◊§◊î", "◊û◊ô ◊§◊î"], "Soap": ["◊°◊ë◊ï◊ü", "◊°◊ë◊ï◊ü ◊ô◊ì◊ô◊ô◊ù", "◊°◊ë◊ï◊ü ◊†◊ï◊ñ◊ú◊ô", "◊°◊ë◊ï◊ü ◊û◊ï◊¶◊ß", "◊°◊ë◊ï◊ü ◊®◊ó◊¶◊î", "◊°◊ë◊ï◊ü ◊ê◊†◊ò◊ô◊ë◊ß◊ò◊®◊ô◊ê◊ú◊ô"],
    "Face Wash": ["◊°◊ë◊ï◊ü ◊§◊†◊ô◊ù", "◊í'◊ú ◊†◊ô◊ß◊ï◊ô", "◊™◊ó◊ú◊ô◊ë ◊†◊ô◊ß◊ï◊ô", "◊†◊ô◊ß◊ï◊ô ◊§◊†◊ô◊ù", "◊û◊ï◊° ◊†◊ô◊ß◊ï◊ô"],
    "Moisturizer": ["◊ß◊®◊ù ◊ú◊ó◊ï◊™", "◊™◊ó◊ú◊ô◊ë ◊ú◊ó◊ï◊™", "◊ß◊®◊ù ◊§◊†◊ô◊ù", "◊ß◊®◊ù ◊í◊ï◊£"], "Sunscreen": ["◊ß◊®◊ù ◊î◊í◊†◊î", "◊û◊ß◊ì◊ù ◊î◊í◊†◊î", "◊î◊í◊†◊î ◊û◊î◊©◊û◊©"],
    "Vitamins": ["◊ï◊ô◊ò◊û◊ô◊†◊ô◊ù", "◊ï◊ô◊ò◊û◊ô◊ü", "◊û◊ï◊ú◊ò◊ô ◊ï◊ô◊ò◊û◊ô◊ü"], "Supplements": ["◊™◊ï◊°◊§◊ô ◊™◊ñ◊ï◊†◊î", "◊™◊ï◊°◊£", "◊§◊®◊ï◊ë◊ô◊ï◊ò◊ô◊ß◊î", "◊ß◊ï◊ú◊í◊ü"],
    "Diapers": ["◊ó◊ô◊™◊ï◊ú◊ô◊ù", "◊ó◊ô◊™◊ï◊ú", "◊ò◊ô◊ò◊ï◊ú◊ô◊ù"], "Baby Wipes": ["◊û◊í◊ë◊ï◊†◊ô◊ù", "◊û◊í◊ë◊ï◊†◊ô◊ù ◊ú◊ó◊ô◊ù", "◊û◊í◊ë◊ï◊†◊ô ◊™◊ô◊†◊ï◊ß"],
    "Baby Formula": ["◊™◊û◊¥◊ú", "◊™◊®◊õ◊ï◊ë◊™ ◊û◊ñ◊ï◊ü", "◊™◊ó◊ú◊ô◊£ ◊ó◊ú◊ë", "◊§◊ï◊®◊û◊ï◊ú◊î"], "Feminine Hygiene": ["◊™◊ó◊ë◊ï◊©◊ï◊™", "◊ò◊û◊§◊ï◊†◊ô◊ù", "◊§◊ì◊ô◊ù ◊ô◊ï◊û◊ô◊ô◊ù", "◊û◊ï◊¶◊®◊ô ◊î◊ô◊í◊ô◊ô◊†◊î ◊†◊©◊ô◊™"],
    "Contraceptives": ["◊ê◊û◊¶◊¢◊ô ◊û◊†◊ô◊¢◊î", "◊ß◊ï◊†◊ì◊ï◊û◊ô◊ù", "◊í◊ú◊ï◊ú◊ï◊™"], "Pregnancy Test": ["◊ë◊ì◊ô◊ß◊™ ◊î◊®◊ô◊ï◊ü"],
    "Thermometer": ["◊û◊ì◊ó◊ï◊ù", "◊û◊ì ◊ó◊ï◊ù"], "Bandages": ["◊§◊ú◊°◊ò◊®◊ô◊ù", "◊§◊ú◊°◊ò◊®", "◊™◊ó◊ë◊ï◊©◊ï◊™", "◊ê◊í◊ì"],
    "Antiseptic": ["◊ó◊ï◊û◊® ◊ó◊ô◊ò◊ï◊ô", "◊§◊ï◊ú◊ô◊ì◊ô◊ü", "◊°◊ë◊ô◊ì◊ô◊ü"], "Hand Sanitizer": ["◊í'◊ú ◊ê◊ú◊õ◊ï◊î◊ï◊ú", "◊ó◊ô◊ò◊ï◊ô ◊ô◊ì◊ô◊ô◊ù", "◊ê◊ú◊õ◊ï◊í'◊ú"],
    "Allergy Medicine": ["◊™◊®◊ï◊§◊î ◊ú◊ê◊ú◊®◊í◊ô◊î", "◊ê◊†◊ò◊ô◊î◊ô◊°◊ò◊û◊ô◊ü"], "Cold Medicine": ["◊™◊®◊ï◊§◊î ◊ú◊î◊¶◊ò◊†◊†◊ï◊™"],
    "Cough Syrup": ["◊°◊ô◊®◊ï◊§ ◊©◊ô◊¢◊ï◊ú"], "Nasal Spray": ["◊°◊§◊®◊ô◊ô ◊ú◊ê◊£", "◊ò◊ô◊§◊ï◊™ ◊ê◊£"],
    "Eye Drops": ["◊ò◊ô◊§◊ï◊™ ◊¢◊ô◊†◊ô◊ô◊ù", "◊ì◊û◊¢◊ï◊™ ◊û◊ú◊ê◊õ◊ï◊™◊ô◊ï◊™"], "Contact Lens Solution": ["◊™◊û◊ô◊°◊î ◊ú◊¢◊ì◊©◊ï◊™", "◊†◊ï◊ñ◊ú ◊¢◊ì◊©◊ï◊™"],
    "Razor": ["◊°◊õ◊ô◊ü ◊í◊ô◊ú◊ï◊ó", "◊°◊õ◊ô◊†◊ô ◊í◊ô◊ú◊ï◊ó"], "Shaving Cream": ["◊ß◊¶◊£ ◊í◊ô◊ú◊ï◊ó", "◊í'◊ú ◊í◊ô◊ú◊ï◊ó", "◊ß◊®◊ù ◊í◊ô◊ú◊ï◊ó"],
    "After Shave": ["◊ê◊§◊ò◊® ◊©◊ô◊ô◊ë"], "Hair Dye": ["◊¶◊ë◊¢ ◊©◊ô◊¢◊®"], "Nail Polish": ["◊ú◊ß", "◊ú◊ß ◊¶◊ô◊§◊ï◊®◊†◊ô◊ô◊ù"],
    "Nail Polish Remover": ["◊û◊°◊ô◊® ◊ú◊ß"], "Makeup Remover": ["◊û◊°◊ô◊® ◊ê◊ô◊§◊ï◊®", "◊û◊ô◊ù ◊û◊ô◊°◊ú◊®◊ô◊ô◊ù"],
    "Foundation": ["◊û◊ô◊ô◊ß ◊ê◊§", "◊§◊ê◊ï◊†◊ì◊ô◊ô◊©◊ü", "◊û◊ô◊ô◊ß◊ê◊§"], "Mascara": ["◊û◊°◊ß◊®◊î"],
    "Lipstick": ["◊©◊§◊™◊ï◊ü", "◊ú◊ô◊§◊°◊ò◊ô◊ß", "◊ê◊ï◊ì◊ù"], "Eyeliner": ["◊ê◊ô◊ô◊ú◊ô◊ô◊†◊®", "◊¢◊ô◊§◊®◊ï◊ü ◊¢◊ô◊†◊ô◊ô◊ù"],
    "Acne Treatment": ["◊ò◊ô◊§◊ï◊ú ◊ë◊ê◊ß◊†◊î"], "Anti-Aging Cream": ["◊ß◊®◊ù ◊ê◊†◊ò◊ô ◊ê◊ô◊ô◊í'◊ô◊†◊í", "◊†◊í◊ì ◊ß◊û◊ò◊ô◊ù"],
    "Serum": ["◊°◊®◊ï◊ù", "◊°◊®◊ï◊ù ◊§◊†◊ô◊ù"], "Face Mask Treatment": ["◊û◊°◊õ◊™ ◊§◊†◊ô◊ù", "◊û◊°◊õ◊™ ◊ë◊ì", "◊û◊°◊õ◊™ ◊ó◊ô◊û◊®"],
    "Pain Reliever": ["◊û◊©◊õ◊ö ◊õ◊ê◊ë◊ô◊ù", "◊ê◊ß◊û◊ï◊ú", "◊†◊ï◊®◊ï◊§◊ü", "◊ê◊ï◊§◊ò◊ú◊í◊ô◊ü"], "Reading Glasses": ["◊û◊©◊ß◊§◊ô ◊ß◊®◊ô◊ê◊î"]
}

# --- Caching Functions ---
def load_cache():
    """Loads the enrichment cache from a JSON file."""
    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

def save_to_cache(cache, key, data):
    """Saves a single entry to the cache and writes the whole cache to disk."""
    cache[key] = data
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

# --- Other Functions (DB, Rules, etc.) ---
def get_db_connection():
    try:
        return psycopg2.connect(host=PG_HOST, port=PG_PORT, database=PG_DATABASE, user=PG_USER, password=PG_PASSWORD)
    except psycopg2.Error as e:
        raise

def get_products_to_enrich(cursor):
    cursor.execute("SELECT product_id, canonical_name, brand FROM products WHERE attributes IS NULL OR attributes->>'product_type' IS NULL ORDER BY product_id;")
    return cursor.fetchall()

def _parse_reading_glasses(product_name: str):
    attributes = {}
    strength_match = re.search(r'\+(\d\.\d{2})', product_name)
    if strength_match:
        attributes['strength'] = f"+{strength_match.group(1)}"
    colors = ['◊©◊ó◊ï◊®', '◊õ◊ó◊ï◊ú', '◊ñ◊î◊ë', '◊ê◊ì◊ï◊ù', '◊ï◊®◊ï◊ì', '◊û◊†◊ï◊û◊®', '◊©◊ß◊ï◊£']
    found_colors = [color for color in colors if color in product_name]
    if found_colors:
        attributes['variant'] = '/'.join(found_colors)
    return attributes

def extract_attributes_with_rules(product_name: str):
    attributes = {}
    best_match_type = None
    longest_match_len = 0
    for p_type, keywords in PRODUCT_TYPE_MAP.items():
        for keyword in keywords:
            pattern = r'(^|\s)' + re.escape(keyword) + r'($|\s)'
            if re.search(pattern, product_name):
                if len(keyword) > longest_match_len:
                    longest_match_len = len(keyword)
                    best_match_type = p_type
    if best_match_type:
        attributes['product_type'] = best_match_type
    SPECIALIZED_PARSERS = {"Reading Glasses": _parse_reading_glasses}
    if best_match_type and best_match_type in SPECIALIZED_PARSERS:
        attributes.update(SPECIALIZED_PARSERS[best_match_type](product_name))
    size_pattern = r'(\d+(?:\.\d+)?)\s?(◊û"◊ú|◊û◊ú|◊í◊®◊ù|◊í"◊®|◊í◊®|◊ú◊ô◊ò◊®|◊ú|◊ß"◊í|◊ß◊í|◊ô◊ó|◊ô◊ó◊ô◊ì◊ï◊™|◊û"◊í|◊û◊í)'
    size_match = re.search(size_pattern, product_name, re.IGNORECASE)
    if size_match:
        attributes['size_value'] = float(size_match.group(1))
        unit = size_match.group(2).lower().replace('"',"").replace("'", "")
        unit_map = {'◊û◊ú': 'ml', '◊û"◊ú': 'ml', '◊í◊®': 'g', '◊í◊®◊ù': 'g', '◊í"◊®': 'g', '◊û◊í': 'mg', '◊û"◊í': 'mg', '◊ú◊ô◊ò◊®': 'l', '◊ú': 'l', '◊ß◊í': 'kg', '◊ß"◊í': 'kg', '◊ô◊ó◊ô◊ì◊ï◊™': 'units', '◊ô◊ó': 'units'}
        attributes['size_unit'] = unit_map.get(unit, unit)
    return attributes if attributes else None

def get_extracted_attributes(product_name: str, brand: str):
    prompt = f"""
    From the Israeli product title below, extract the specified attributes into a valid JSON object.
    - product_type: The generic category of the item (e.g., "Shampoo", "Conditioner", "Body Lotion").
    - variant: The specific version, scent, or key ingredient (e.g., "Argan Oil", "Coconut Milk", "For Colored Hair").
    - size_value: The numerical size or amount. If not present, use null.
    - size_unit: The unit of measurement (e.g., "ml", "g", "l", "units"). If not present, use null.
    Product Title: "{product_name}"
    Brand: "{brand}"
    JSON:
    """
    try:
        response = openai.chat.completions.create(model="gpt-4o", messages=[{"role": "system", "content": "You are a highly accurate data extraction assistant. Your only output is a single, valid JSON object."}, {"role": "user", "content": prompt}], response_format={"type": "json_object"}, temperature=0.0)
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"  -> ERROR: OpenAI API call failed for '{product_name}': {e}")
        return None

def update_product_attributes(cursor, product_id, new_attributes):
    # THE CRITICAL SQL FIX IS HERE: COALESCE ensures we don't merge with NULL.
    cursor.execute("UPDATE products SET attributes = COALESCE(attributes, '{}'::jsonb) || %s WHERE product_id = %s;", (Json(new_attributes), product_id))

# --- Main Execution ---
def main():
    parser = argparse.ArgumentParser(description="Enrich product data using rules and an LLM.")
    parser.add_argument('--dry-run', action='store_true', help="Run without saving changes to the database.")
    parser.add_argument('--review-file', type=str, help="Save all extraction results to a CSV file for review.")
    args = parser.parse_args()

    if args.dry_run:
        print("--- üî¨ Starting Product Enrichment DRY RUN ---")
    else:
        print("--- üß† Starting Product Enrichment Process ---")

    # Load the cache at the beginning
    enrichment_cache = load_cache()
    print(f"-> Loaded {len(enrichment_cache)} items from local cache.")

    review_file, csv_writer = None, None
    if args.review_file:
        try:
            review_file = open(args.review_file, 'w', newline='', encoding='utf-8-sig')
            csv_writer = csv.writer(review_file)
            csv_writer.writerow(['product_id', 'original_name', 'source', 'product_type', 'variant', 'size_value', 'size_unit', 'strength'])
        except IOError as e:
            print(f"[ERROR] Could not open review file: {e}"); return

    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cursor:
            products = get_products_to_enrich(cursor)
            if not products:
                print("‚úÖ All products are already enriched. Nothing to do."); return

            print(f"Found {len(products)} products to process...")
            for i, (product_id, name, brand) in enumerate(products):
                print(f"[{i+1}/{len(products)}] Processing: '{name}'")
                
                source = "Rules"
                attributes = extract_attributes_with_rules(name)

                if not attributes:
                    source = "LLM"
                    # Check cache before calling API
                    if name in enrichment_cache:
                        attributes = enrichment_cache[name]
                        print("  -> SUCCESS (Cache): Found in local cache.")
                    else:
                        print("  -> Rules failed, falling back to LLM...")
                        attributes = get_extracted_attributes(name, brand)
                        if attributes:
                            # Save to cache immediately after successful API call
                            save_to_cache(enrichment_cache, name, attributes)
                
                if attributes:
                    print(f"  -> SUCCESS ({source}): Extracted {attributes}")
                    if not args.dry_run:
                        update_product_attributes(cursor, product_id, attributes)
                    if csv_writer:
                        row = [product_id, name, source, attributes.get('product_type', ''), attributes.get('variant', ''), attributes.get('size_value', ''), attributes.get('size_unit', ''), attributes.get('strength', '')]
                        csv_writer.writerow(row)
                else:
                    print("  -> SKIPPED: All extraction methods failed.")

                if not args.dry_run and (i + 1) % BATCH_SIZE == 0:
                    conn.commit()
                    print(f"--- üíæ Committed batch of {BATCH_SIZE} products. Progress saved. ---")

            if not args.dry_run:
                conn.commit()
                print("\n--- ‚úÖ Enrichment Complete. Database updated. ---")
            else:
                print("\n--- üî¨ Dry run complete. No changes were saved. ---")

    except (Exception, KeyboardInterrupt) as e:
        if conn and not args.dry_run:
            print("\n--- ‚ö†Ô∏è Process interrupted. Rolling back current batch. ---")
            conn.rollback()
        print(f"\n[FATAL ERROR] An error occurred: {e}")
    finally:
        if conn: conn.close()
        if review_file: review_file.close()

if __name__ == "__main__":
    main()
